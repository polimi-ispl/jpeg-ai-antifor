{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a748b3b0",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "A first exploration of the datasets created compressing pristine images from various SOTA papers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8f9292",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c0014",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a002415",
   "metadata": {},
   "source": [
    "## Let's find all the images inside the various folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd86dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T08:01:38.376026Z",
     "start_time": "2024-10-16T07:14:36.765833Z"
    }
   },
   "outputs": [],
   "source": [
    "if os.path.exists('/nas/public/exchange/JPEG-AI/data/TEST/data_info.csv'):\n",
    "    # Load the CSV directly\n",
    "    all_images = pd.read_csv('/nas/public/exchange/JPEG-AI/data/TEST/data_info.csv')\n",
    "else:\n",
    "    # Look for all the images inside the directory (avoid binary files)\n",
    "    all_images = pd.DataFrame([path for path in glob.glob(os.path.join('/nas/public/exchange/JPEG-AI/data/TEST/**/*.*'), recursive=True) if 'ipynb' not in path], columns=['path'])\n",
    "    # add other useful info\n",
    "    all_images['dataset'] = all_images['path'].apply(lambda x: x.split('/')[7])\n",
    "    all_images['compressed'] = all_images['path'].apply(lambda x: True if 'compressed' in x else False)\n",
    "    all_images['target_bpp'] = all_images['path'].apply(lambda x: x.split('target_bpp_')[1].split('/')[0] if 'target_bpp' in x else None)\n",
    "    all_images['target_bpp'] = all_images['target_bpp'].apply(lambda x: float(x)/100 if x is not None else None)\n",
    "    all_images['filename'] = all_images['path'].apply(lambda x: os.path.basename(x))\n",
    "    content_dict = {'imagenet': 'various', 'celeba': 'faces', 'ffhq': 'faces', 'coco': 'various', 'raise': 'various', 'laion': 'various'}\n",
    "    all_images['content'] = all_images['dataset'].apply(lambda x: content_dict[x] if 'lsun' not in x else None)\n",
    "    # Fix LSUN\n",
    "    for i, r in all_images.loc[all_images['dataset']=='lsun'].iterrows():\n",
    "        if r['compressed']:\n",
    "            all_images.loc[i, 'content'] = r['path'].split('/')[10]\n",
    "        else:\n",
    "            all_images.loc[i, 'content'] = r['path'].split('/')[9]\n",
    "    # Let's add info on the single image sizes\n",
    "    all_images['size'] = all_images['path'].apply(lambda x: Image.open(x).size)\n",
    "    # Let's save the data into a csv\n",
    "    all_images.to_csv('/nas/public/exchange/JPEG-AI/data/TEST/data_info.csv', index=False)\n",
    "all_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d1d29c",
   "metadata": {},
   "source": [
    "## Let's plot some info about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1db3eec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T08:01:38.377057Z",
     "start_time": "2024-10-16T07:14:37.280998Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Number of samples divided by dataset\n",
    "all_images.groupby('dataset').count()['path'].plot.bar(figsize=(12, 9)), plt.title('Number of samples per dataset'), plt.show()\n",
    "\n",
    "# Number of compressed samples\n",
    "all_images.groupby('compressed').count()['content'].plot.bar(figsize=(12, 9)), plt.title('Number of compressed samples'), plt.show()\n",
    "\n",
    "# Number of samples divided by semantic category\n",
    "all_images.groupby('content').count()['path'].plot.bar(figsize=(12, 9)), plt.title('Number of samples per category'), plt.show()\n",
    "\n",
    "# Number of samples divided by target BPP\n",
    "all_images.loc[all_images['compressed']].groupby('target_bpp').count()['path'].plot.bar(figsize=(12, 9)), plt.title('Number of samples per BPP'), plt.show()\n",
    "\n",
    "# Number of samples divided by size\n",
    "all_images.groupby('size').count()['dataset'].plot.bar(figsize=(12, 9)), plt.title('Number of samples per size'), plt.show()\n",
    "\n",
    "# Number of samples divided by size and dataset\n",
    "all_images.groupby(['size', 'dataset']).count()['path'].unstack().plot.bar(figsize=(12, 9)), plt.title('Number of samples per size and dataset'), plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0e24b0",
   "metadata": {},
   "source": [
    "## Great! Now let's look at some quality metrics\n",
    "# THIS TAKES TOO LONG!\n",
    "I computed it offline and placed it inside a different folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9efed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append('/nas/home/ecannas/third_party_code/jpeg-ai-reference-software')\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "# from src.codec.metrics.metrics import DataClass, MetricsProcessor, MetricsFabric\n",
    "# from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1c9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare the metrics processor\n",
    "# metrics = MetricsProcessor()\n",
    "# metrics.internal_bits = 10\n",
    "# metrics.jvet_psnr = False\n",
    "# metrics.metrics = MetricsFabric.metrics_list\n",
    "# metrics.metrics_output = [metric for metric in MetricsFabric.metrics_list]\n",
    "# metrics.color_conv = '709'\n",
    "# metrics.max_samples_for_eval_on_gpu = -1\n",
    "# gpu = 3\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faf5b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dict = []\n",
    "# for dataset in all_images['dataset'].unique():\n",
    "#     print(f'Doing dataset {dataset}...')\n",
    "#     # select the images from the dataset\n",
    "#     images_df = all_images.loc[all_images['dataset']==dataset]\n",
    "#     orig_df = images_df.loc[~images_df['compressed']].iloc[:5]\n",
    "#     compr_df = images_df.loc[images_df['compressed']]\n",
    "#     \n",
    "#     # Create the metrics Dataframe\n",
    "#     images_dict = []\n",
    "#     \n",
    "#     # Cycle over the different pristine samples\n",
    "#     for i, r in tqdm(orig_df.iterrows()):\n",
    "#         # Load the original sample\n",
    "#         filename, dataset, content = r['filename'], r['dataset'], r['content']\n",
    "#         orig_sample, _ = DataClass().load_image(r['path'], color_conv='709', device='cuda')\n",
    "#         # Find the corresponding compressed samples\n",
    "#         comp_samples = compr_df.loc[(compr_df['filename']==filename.replace('jpg', 'png')) \\\n",
    "#                                     & (compr_df['dataset']==dataset) \\\n",
    "#                                     & (compr_df['content']==content)]\n",
    "#         # Cycle over the various BPPs values\n",
    "#         bpps_dict = []\n",
    "#         for ii, rr in comp_samples.iterrows():\n",
    "#             # Load the compressed samples\n",
    "#             comp_sample, _ = DataClass().load_image(rr['path'], color_conv='709', device='cuda')\n",
    "#             # Compute the metrics\n",
    "#             metrics_vals = metrics.process_images(orig_sample, comp_sample)\n",
    "#             # Save them\n",
    "#             bpps_dict.append(pd.DataFrame.from_dict({rr['target_bpp']: {metric: metrics_vals[idx] for idx, metric in enumerate(metrics.metrics_output)}},\n",
    "#                                                     orient='index'))\n",
    "#         \n",
    "#         # Append the image path to the Dataframe and save it\n",
    "#         images_dict.append(pd.concat({r['path']: pd.concat(bpps_dict)}, names=['path', 'bpp']))\n",
    "#     \n",
    "#     # Append the images dictionary\n",
    "#     dataset_dict.append(pd.concat({dataset: pd.concat(images_dict)}, names=['dataset', 'path', 'bpp']))\n",
    "# \n",
    "#             \n",
    "#         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0163449f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T08:01:38.377483Z",
     "start_time": "2024-10-16T07:45:56.943953Z"
    }
   },
   "outputs": [],
   "source": [
    "# metrics_info = pd.concat(dataset_dict)\n",
    "# metrics_info\n",
    "metrics_info = pd.read_csv('/nas/public/exchange/JPEG-AI/code/quality_report/quality_report.csv')\n",
    "print(metrics_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283c7263",
   "metadata": {},
   "source": [
    "## Let's plot some graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52756cd92b3332a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5217d3af711727",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T08:01:38.377645Z",
     "start_time": "2024-10-16T07:18:37.072801Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use path as the index\n",
    "metrics_info.set_index('path', inplace=True)\n",
    "# Add the BPPs to the index\n",
    "metrics_info.set_index('bpp', append=True, inplace=True)\n",
    "# Add the dataset to the index\n",
    "metrics_info.set_index('dataset', append=True, inplace=True)\n",
    "metrics_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006e443d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-16T08:01:38.378310Z",
     "start_time": "2024-10-16T07:18:42.034059Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels_dict = {'msssim_torch': 'Multi-scale SSIM (MS-SSIM)',\n",
    "  'msssim_iqa': 'Multi-scale SSIM IQA',\n",
    "  'psnr': 'Peak Signal-to-Noise Ratio (PSNR)',\n",
    "  'vif': 'Visual Information Fidelity (VIF)',\n",
    "  'fsim': 'Feature Similarity (FSIM)',\n",
    "  'nlpd': 'Normalized Laplacian Pyramid (NLPD)',\n",
    "  'iw-ssim': 'IW-SSIM',\n",
    "  'vmaf': ' Video Multimethod Assessment Fusion (VMAF)',\n",
    "  'psnr_hvs': 'Peak Signal-to-Noise Ratio (PSNR) in the HSV space'}\n",
    "\n",
    "ylabel_dict = {'msssim_torch': 'MS-SSIM [0-1]',\n",
    "  'msssim_iqa': 'MS-SSIM [0-1]',\n",
    "  'psnr': 'PSNR [dB]',\n",
    "  'vif': 'VIF',\n",
    "  'fsim': 'FSIM',\n",
    "  'nlpd': 'NLPD [0-1]',\n",
    "  'iw-ssim': 'IW-SSIM [0-1]',\n",
    "  'vmaf': ' VMAF [0-1]',\n",
    "  'psnr_hvs': 'PSNR-HSV [dB]'}\n",
    "\n",
    "# Let's plot the single metrics\n",
    "for metric in metrics_info.columns:\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(12, 3))\n",
    "    for dataset in metrics_info.index.get_level_values(2).unique():\n",
    "        dataset_info = metrics_info.swaplevel(0, 2).loc[dataset]\n",
    "        avg_metrics = dataset_info.groupby('bpp').mean()[metric]\n",
    "        avg_metrics.sort_index(inplace=True)\n",
    "        axs.plot(avg_metrics, label=dataset)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.title(f'{labels_dict[metric]} results at various BPPs')\n",
    "    plt.ylabel(ylabel_dict[metric])\n",
    "    plt.xticks(ticks=np.linspace(avg_metrics.index[0], avg_metrics.index[-1], len(avg_metrics.index)),\n",
    "               labels=avg_metrics.index.tolist())\n",
    "    plt.xlabel('Bit-Per-Pixels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af490168",
   "metadata": {},
   "source": [
    "Let's have another plot where we put the absolute scale for the reference metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b4c223",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "labels_dict = {'msssim_torch': 'Multi-scale SSIM (MS-SSIM)',\n",
    "  'msssim_iqa': 'Multi-scale SSIM IQA',\n",
    "  'psnr': 'Peak Signal-to-Noise Ratio (PSNR)',\n",
    "  'vif': 'Visual Information Fidelity (VIF)',\n",
    "  'fsim': 'Feature Similarity (FSIM)',\n",
    "  'nlpd': 'Normalized Laplacian Pyramid (NLPD)',\n",
    "  'iw-ssim': 'IW-SSIM',\n",
    "  'vmaf': ' Video Multimethod Assessment Fusion (VMAF)',\n",
    "  'psnr_hvs': 'Peak Signal-to-Noise Ratio (PSNR) in the HSV space'}\n",
    "\n",
    "ylabel_dict = {'msssim_torch': 'MS-SSIM [0-1]',\n",
    "  'msssim_iqa': 'MS-SSIM [0-1]',\n",
    "  'psnr': 'PSNR [dB]',\n",
    "  'vif': 'VIF',\n",
    "  'fsim': 'FSIM',\n",
    "  'nlpd': 'NLPD [0-1]',\n",
    "  'iw-ssim': 'IW-SSIM [0-1]',\n",
    "  'vmaf': ' VMAF [0-1]',\n",
    "  'psnr_hvs': 'PSNR-HSV [dB]'}\n",
    "\n",
    "scale_dict = {'msssim_torch': [0, 1],\n",
    "  'msssim_iqa': [0, 1],\n",
    "  'psnr': [0, 51],\n",
    "  'vif': [0, 56],\n",
    "  'fsim': [0, 56],\n",
    "  'nlpd': [0, 1],\n",
    "  'iw-ssim': [0, 1],\n",
    "  'vmaf': [0, 1],\n",
    "  'psnr_hvs': [0, 1]}\n",
    "\n",
    "# Let's plot the single metrics\n",
    "for metric in metrics_info.columns:\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(12, 3))\n",
    "    for dataset in metrics_info.index.get_level_values(2).unique():\n",
    "        dataset_info = metrics_info.swaplevel(0, 2).loc[dataset]\n",
    "        avg_metrics = dataset_info.groupby('bpp').mean()[metric]\n",
    "        avg_metrics.sort_index(inplace=True)\n",
    "        axs.plot(avg_metrics, label=dataset)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.title(f'{labels_dict[metric]} results at various BPPs')\n",
    "    plt.ylabel(ylabel_dict[metric])\n",
    "    plt.ylim(scale_dict[metric])\n",
    "    plt.xticks(ticks=np.linspace(avg_metrics.index[0], avg_metrics.index[-1], len(avg_metrics.index)),\n",
    "               labels=avg_metrics.index.tolist())\n",
    "    plt.xlabel('Bit-Per-Pixels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7740403",
   "metadata": {},
   "source": [
    "## How bad do the samples look for the different datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84ad5d3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fsize=20\n",
    "for dataset in all_images['dataset'].unique():\n",
    "    # Select the dataset\n",
    "    dataset_df = all_images.loc[all_images['dataset']==dataset]\n",
    "    # Get the uncompressed sample\n",
    "    unc_df = dataset_df.loc[dataset_df['compressed']==False]\n",
    "    sample = unc_df.sample(1, random_state=42)\n",
    "    # Get the compressed samples\n",
    "    comp_df = dataset_df.loc[dataset_df['compressed']]\n",
    "    comp_samples = comp_df.loc[(comp_df['filename']==sample['filename'].item().replace('jpg', 'png')) & (comp_df['content']==sample['content'].item())]\n",
    "    # Plot everything\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(9*3, 9*2))\n",
    "    for idx, (i, r) in enumerate(comp_samples.iterrows()):\n",
    "        row_idx = (idx+1)//3\n",
    "        col_idx = idx if i <3 else idx-3+1\n",
    "        axs[row_idx][col_idx].imshow(Image.open(r['path']).convert('RGB'))\n",
    "        axs[row_idx][col_idx].axis('off')\n",
    "        axs[row_idx][col_idx].set_title(f'{r[\"target_bpp\"]} BPP', fontsize=fsize-5)\n",
    "    axs[0][0].imshow(Image.open(sample['path'].item()).convert('RGB')) \n",
    "    axs[0][0].axis('off')\n",
    "    axs[0][0].set_title('Uncompressed sample', fontsize=fsize-5)\n",
    "    fig.suptitle(f'{dataset} sample', fontsize=fsize)\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883e30f0",
   "metadata": {},
   "source": [
    "### Let's look at the center crop in 224x224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c39a9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fsize=20\n",
    "for dataset in all_images['dataset'].unique():\n",
    "    # Select the dataset\n",
    "    dataset_df = all_images.loc[all_images['dataset']==dataset]\n",
    "    # Get the uncompressed sample\n",
    "    unc_df = dataset_df.loc[dataset_df['compressed']==False]\n",
    "    sample = unc_df.sample(1, random_state=42)\n",
    "    # Get the compressed samples\n",
    "    comp_df = dataset_df.loc[dataset_df['compressed']]\n",
    "    comp_samples = comp_df.loc[(comp_df['filename']==sample['filename'].item().replace('jpg', 'png')) & (comp_df['content']==sample['content'].item())]\n",
    "    # Get the crop coordinates for a 224x224 patch\n",
    "    height, width = int(sample['size'].item().split('(')[1].split(',')[0]), int(sample['size'].item().split(')')[0].split(', ')[1]) \n",
    "    left = (width - 224)/2\n",
    "    top = (height - 224)/2\n",
    "    right = (width + 224)/2\n",
    "    bottom = (height + 224)/2\n",
    "    # Plot everything\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(9*3, 9*2))\n",
    "    for idx, (i, r) in enumerate(comp_samples.iterrows()):\n",
    "        row_idx = (idx+1)//3\n",
    "        col_idx = idx if i <3 else idx-3+1\n",
    "        axs[row_idx][col_idx].imshow(Image.open(r['path']).convert('RGB').crop((left, top, right, bottom)))\n",
    "        axs[row_idx][col_idx].axis('off')\n",
    "        axs[row_idx][col_idx].set_title(f'{r[\"target_bpp\"]} BPP', fontsize=fsize-5)\n",
    "    axs[0][0].imshow(Image.open(sample['path'].item()).convert('RGB').crop((left, top, right, bottom))) \n",
    "    axs[0][0].axis('off')\n",
    "    axs[0][0].set_title('Uncompressed sample', fontsize=fsize-5)\n",
    "    fig.suptitle(f'{dataset} sample', fontsize=fsize)\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53329fe2",
   "metadata": {},
   "source": [
    "### Are there some more visible artifacts in some datasets than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c54f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def compute_spectrum_module(img: Image) -> np.ndarray:\n",
    "    # Convert the image to grayscale\n",
    "    img = np.array(img.convert('L'))\n",
    "    \n",
    "    # Create the median blurred image\n",
    "    blur_img = cv2.medianBlur(img, 5)\n",
    "    \n",
    "    # High pass filtering\n",
    "    hp_img = cv2.subtract(img, blur_img)\n",
    "    \n",
    "    # Compute the FFT\n",
    "    fft_img = np.fft.fftshift(np.fft.fft2(hp_img))\n",
    "    \n",
    "    # Return the module\n",
    "    return np.abs(fft_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438fcf0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fsize=20\n",
    "for dataset in all_images['dataset'].unique():\n",
    "    # Select the dataset\n",
    "    dataset_df = all_images.loc[all_images['dataset']==dataset]\n",
    "    # Get the uncompressed sample\n",
    "    unc_df = dataset_df.loc[dataset_df['compressed']==False]\n",
    "    sample = unc_df.sample(1, random_state=42)\n",
    "    # Get the compressed samples\n",
    "    comp_df = dataset_df.loc[dataset_df['compressed']]\n",
    "    comp_samples = comp_df.loc[(comp_df['filename']==sample['filename'].item().replace('jpg', 'png')) & (comp_df['content']==sample['content'].item())]\n",
    "    # Get the crop coordinates for a 224x224 patch\n",
    "    height, width = int(sample['size'].item().split('(')[1].split(',')[0]), int(sample['size'].item().split(')')[0].split(', ')[1]) \n",
    "    left = (width - 224)/2\n",
    "    top = (height - 224)/2\n",
    "    right = (width + 224)/2\n",
    "    bottom = (height + 224)/2\n",
    "    # Plot everything\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(9*3, 9*2))\n",
    "    for idx, (i, r) in enumerate(comp_samples.iterrows()):\n",
    "        row_idx = (idx+1)//3\n",
    "        col_idx = idx if i <3 else idx-3+1\n",
    "        spec = compute_spectrum_module(Image.open(r['path']).convert('RGB').crop((left, top, right, bottom)))\n",
    "        qmin, qmax = np.quantile(spec.flatten(), [0.5, 0.999])\n",
    "        #axs[row_idx][col_idx].imshow(spec, vmin=qmin, vmax=qmax)\n",
    "        axs[row_idx][col_idx].imshow(np.log(1+spec))\n",
    "        axs[row_idx][col_idx].axis('off')\n",
    "        axs[row_idx][col_idx].set_title(f'{r[\"target_bpp\"]} BPP spectrum', fontsize=fsize-5)\n",
    "    spec = compute_spectrum_module(Image.open(r['path']).convert('RGB').crop((left, top, right, bottom)))\n",
    "    #qmin, qmax = np.quantile(spec.flatten(), [0.5, 0.999])\n",
    "    #axs[0][0].imshow(spec, vmin=qmin, vmax=qmax) \n",
    "    axs[0][0].imshow(np.log(1+spec))\n",
    "    axs[0][0].axis('off')\n",
    "    axs[0][0].set_title('Uncompressed sample spectrum', fontsize=fsize-5)\n",
    "    fig.suptitle(f'{dataset} sample', fontsize=fsize)\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868c6bb6",
   "metadata": {},
   "source": [
    "## Does not seem to be a clear pattern here...\n",
    "Let's try to compute the average spectrum from multiple images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af20ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from typing import Tuple\n",
    "\n",
    "# --- THIS IS THE OHJA IMPLEMENTATION ACCORDING TO THEIR PAPER, DOES NOT MAKE SENSE TO ME\n",
    "# def compute_avg_spectrum_module(df: pd.DataFrame) -> np.ndarray:\n",
    "    \n",
    "#     avg_spec = []\n",
    "#     for i, r in df.iterrows():\n",
    "        \n",
    "#         # Convert the image to grayscale\n",
    "#         img = Image.open(r['path']).convert('L')\n",
    "        \n",
    "#         # Crop the image\n",
    "#         height, width = img.size\n",
    "#         left = (width - 224)/2\n",
    "#         top = (height - 224)/2\n",
    "#         right = (width + 224)/2\n",
    "#         bottom = (height + 224)/2\n",
    "#         img = np.array(img.crop((left, top, right, bottom)))\n",
    "\n",
    "#         # Create the median blurred image\n",
    "#         blur_img = cv2.medianBlur(img, 5)\n",
    "\n",
    "#         # High pass filtering\n",
    "#         hp_img = cv2.subtract(img, blur_img)\n",
    "        \n",
    "#         # Remove DC component\n",
    "#         #hp_img = cv2.subtract(hp_img, hp_img.mean())\n",
    "\n",
    "#         # Compute the FFT\n",
    "#         avg_spec.append(hp_img[np.newaxis, :, :])\n",
    "    \n",
    "#     # Compute the average\n",
    "#     avg_spec = np.squeeze(np.mean(avg_spec, axis=0))\n",
    "    \n",
    "#     # Compute the spectrum\n",
    "#     avg_spec = np.fft.fftshift(np.fft.fft2(avg_spec))\n",
    "    \n",
    "#     # Return the module\n",
    "#     return np.abs(avg_spec)\n",
    "\n",
    "def compute_avg_spectrum_module(df: pd.DataFrame) -> np.ndarray:\n",
    "    \n",
    "    avg_spec = []\n",
    "    for i, r in df.iterrows():\n",
    "        \n",
    "        # Convert the image to grayscale\n",
    "        img = Image.open(r['path']).convert('L')\n",
    "        \n",
    "        # Crop the image\n",
    "        height, width = img.size\n",
    "        left = (width - 224)/2\n",
    "        top = (height - 224)/2\n",
    "        right = (width + 224)/2\n",
    "        bottom = (height + 224)/2\n",
    "        img = np.array(img.crop((left, top, right, bottom)))\n",
    "\n",
    "        # Create the median blurred image\n",
    "        blur_img = cv2.medianBlur(img, 5)\n",
    "\n",
    "        # High pass filtering\n",
    "        hp_img = cv2.subtract(img, blur_img)\n",
    "        \n",
    "        # Remove DC component\n",
    "        #hp_img = cv2.subtract(hp_img, hp_img.mean())\n",
    "\n",
    "        # Compute the FFT\n",
    "        spec = np.abs(np.fft.fftshift(np.fft.fft2(hp_img)))\n",
    "        avg_spec.append(spec[np.newaxis, :, :])\n",
    "    \n",
    "    # Compute the average\n",
    "    avg_spec = np.squeeze(np.mean(avg_spec, axis=0))\n",
    "    \n",
    "    # Return the module\n",
    "    return avg_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb757c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "fsize=20\n",
    "avg_specs = dict()\n",
    "for dataset in tqdm(all_images['dataset'].unique()):\n",
    "    # Select the dataset\n",
    "    dataset_df = all_images.loc[all_images['dataset']==dataset]\n",
    "    # Get the uncompressed sample\n",
    "    unc_df = dataset_df.loc[dataset_df['compressed']==False]\n",
    "    sample = unc_df.sample(50, random_state=42)\n",
    "    # Get the compressed samples\n",
    "    comp_df = dataset_df.loc[dataset_df['compressed']]\n",
    "    # Merge the dataframes on 'filename' and 'content'\n",
    "    sample['filename'] = sample['filename'].apply(lambda x: x.replace('jpg', 'png'))\n",
    "    comp_samples = pd.merge(comp_df, sample, left_on=['filename', 'content'], right_on=['filename', 'content'],\n",
    "                        suffixes=('', '_uncompressed'))\n",
    "    # Compute them\n",
    "    avg_specs[dataset] = dict()\n",
    "    # Compressed samples\n",
    "    for idx, t_bpp in enumerate(comp_samples['target_bpp'].unique()):\n",
    "        row_idx = (idx+1)//3\n",
    "        col_idx = idx+1 if idx+1 < 3 else idx-3+1\n",
    "        spec = compute_avg_spectrum_module(comp_samples.loc[comp_samples['target_bpp']==t_bpp])\n",
    "        avg_specs[dataset][t_bpp] = spec\n",
    "    # Uncompressed samples\n",
    "    spec = compute_avg_spectrum_module(sample)\n",
    "    avg_specs[dataset]['unc'] = spec\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0b7c1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fsize = 20\n",
    "for dataset, specs in avg_specs.items():\n",
    "    # Prepare the figure\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(9*3, 9*2))\n",
    "    for idx, t_bpp in enumerate(comp_samples['target_bpp'].unique()):\n",
    "        row_idx = (idx+1)//3\n",
    "        col_idx = idx+1 if idx+1 < 3 else idx-3+1\n",
    "        qmin, qmax = np.quantile(specs[t_bpp].flatten(), [0.01, 0.999])\n",
    "        axs[row_idx][col_idx].imshow(specs[t_bpp], vmax=qmax)\n",
    "        #axs[row_idx][col_idx].imshow(np.log(1+specs[t_bpp]))\n",
    "        axs[row_idx][col_idx].axis('off')\n",
    "        axs[row_idx][col_idx].set_title(f'{t_bpp} BPP avg spectrum', fontsize=fsize-5)\n",
    "    qmin, qmax = np.quantile(specs['unc'].flatten(), [0.01, 0.999])\n",
    "    axs[0][0].imshow(specs['unc'], vmax=qmax)\n",
    "    #axs[0][0].imshow(np.log(1+specs['unc']))\n",
    "    axs[0][0].axis('off')\n",
    "    axs[0][0].set_title(f'Uncompressed avg spectrum', fontsize=fsize-5)\n",
    "    fig.suptitle(f'{dataset} average spectrums', fontsize=fsize)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ad8f1d",
   "metadata": {},
   "source": [
    "### Let's try again but with Laplace filtering instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc3c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from typing import Tuple\n",
    "\n",
    "# --- THIS IS THE OHJA IMPLEMENTATION ACCORDING TO THEIR PAPER, DOES NOT MAKE SENSE TO ME\n",
    "# def compute_avg_spectrum_module(df: pd.DataFrame) -> np.ndarray:\n",
    "    \n",
    "#     avg_spec = []\n",
    "#     for i, r in df.iterrows():\n",
    "        \n",
    "#         # Convert the image to grayscale\n",
    "#         img = Image.open(r['path']).convert('L')\n",
    "        \n",
    "#         # Crop the image\n",
    "#         height, width = img.size\n",
    "#         left = (width - 224)/2\n",
    "#         top = (height - 224)/2\n",
    "#         right = (width + 224)/2\n",
    "#         bottom = (height + 224)/2\n",
    "#         img = np.array(img.crop((left, top, right, bottom)))\n",
    "\n",
    "#         # Create the median blurred image\n",
    "#         blur_img = cv2.medianBlur(img, 5)\n",
    "\n",
    "#         # High pass filtering\n",
    "#         hp_img = cv2.subtract(img, blur_img)\n",
    "        \n",
    "#         # Remove DC component\n",
    "#         #hp_img = cv2.subtract(hp_img, hp_img.mean())\n",
    "\n",
    "#         # Compute the FFT\n",
    "#         avg_spec.append(hp_img[np.newaxis, :, :])\n",
    "    \n",
    "#     # Compute the average\n",
    "#     avg_spec = np.squeeze(np.mean(avg_spec, axis=0))\n",
    "    \n",
    "#     # Compute the spectrum\n",
    "#     avg_spec = np.fft.fftshift(np.fft.fft2(avg_spec))\n",
    "    \n",
    "#     # Return the module\n",
    "#     return np.abs(avg_spec)\n",
    "\n",
    "def compute_avg_spectrum_module(df: pd.DataFrame) -> np.ndarray:\n",
    "    \n",
    "    avg_spec = []\n",
    "    for i, r in df.iterrows():\n",
    "        \n",
    "        # Convert the image to grayscale\n",
    "        img = Image.open(r['path']).convert('L')\n",
    "        \n",
    "        # Crop the image\n",
    "        height, width = img.size\n",
    "        left = (width - 224)/2\n",
    "        top = (height - 224)/2\n",
    "        right = (width + 224)/2\n",
    "        bottom = (height + 224)/2\n",
    "        img = np.array(img.crop((left, top, right, bottom)))\n",
    "\n",
    "        # High pass filtering\n",
    "        hp_img = cv2.Laplacian(img, cv2.CV_64F)\n",
    "        \n",
    "        # Remove DC component\n",
    "        #hp_img = cv2.subtract(hp_img, hp_img.mean())\n",
    "\n",
    "        # Compute the FFT\n",
    "        spec = np.abs(np.fft.fftshift(np.fft.fft2(hp_img)))\n",
    "        avg_spec.append(spec[np.newaxis, :, :])\n",
    "    \n",
    "    # Compute the average\n",
    "    avg_spec = np.squeeze(np.mean(avg_spec, axis=0))\n",
    "    \n",
    "    # Return the module\n",
    "    return avg_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d172ba32",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "fsize=20\n",
    "avg_specs = dict()\n",
    "for dataset in tqdm(all_images['dataset'].unique()):\n",
    "    # Select the dataset\n",
    "    dataset_df = all_images.loc[all_images['dataset']==dataset]\n",
    "    # Get the uncompressed sample\n",
    "    unc_df = dataset_df.loc[dataset_df['compressed']==False]\n",
    "    sample = unc_df.sample(50, random_state=42)\n",
    "    # Get the compressed samples\n",
    "    comp_df = dataset_df.loc[dataset_df['compressed']]\n",
    "    # Merge the dataframes on 'filename' and 'content'\n",
    "    sample['filename'] = sample['filename'].apply(lambda x: x.replace('jpg', 'png'))\n",
    "    comp_samples = pd.merge(comp_df, sample, left_on=['filename', 'content'], right_on=['filename', 'content'],\n",
    "                        suffixes=('', '_uncompressed'))\n",
    "    # Compute them\n",
    "    avg_specs[dataset] = dict()\n",
    "    # Compressed samples\n",
    "    for idx, t_bpp in enumerate(comp_samples['target_bpp'].unique()):\n",
    "        row_idx = (idx+1)//3\n",
    "        col_idx = idx+1 if idx+1 < 3 else idx-3+1\n",
    "        spec = compute_avg_spectrum_module(comp_samples.loc[comp_samples['target_bpp']==t_bpp])\n",
    "        avg_specs[dataset][t_bpp] = spec\n",
    "    # Uncompressed samples\n",
    "    spec = compute_avg_spectrum_module(sample)\n",
    "    avg_specs[dataset]['unc'] = spec\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91983762",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fsize = 20\n",
    "for dataset, specs in avg_specs.items():\n",
    "    # Prepare the figure\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(9*3, 9*2))\n",
    "    for idx, t_bpp in enumerate(comp_samples['target_bpp'].unique()):\n",
    "        row_idx = (idx+1)//3\n",
    "        col_idx = idx+1 if idx+1 < 3 else idx-3+1\n",
    "        qmin, qmax = np.quantile(specs[t_bpp].flatten(), [0.01, 0.999])\n",
    "        axs[row_idx][col_idx].imshow(specs[t_bpp], vmax=qmax)\n",
    "        #axs[row_idx][col_idx].imshow(np.log(1+specs[t_bpp]))\n",
    "        axs[row_idx][col_idx].axis('off')\n",
    "        axs[row_idx][col_idx].set_title(f'{t_bpp} BPP avg spectrum', fontsize=fsize-5)\n",
    "    qmin, qmax = np.quantile(specs['unc'].flatten(), [0.01, 0.999])\n",
    "    axs[0][0].imshow(specs['unc'], vmax=qmax)\n",
    "    #axs[0][0].imshow(np.log(1+specs['unc']))\n",
    "    axs[0][0].axis('off')\n",
    "    axs[0][0].set_title(f'Uncompressed avg spectrum', fontsize=fsize-5)\n",
    "    fig.suptitle(f'{dataset} average spectrums', fontsize=fsize)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f851b",
   "metadata": {},
   "source": [
    "### These figures don't convince me a lot... Can we compute the residual using a DnCNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e342724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from typing import Tuple\n",
    "import torch\n",
    "\n",
    "# --- THIS IS THE OHJA IMPLEMENTATION ACCORDING TO THEIR PAPER, DOES NOT MAKE SENSE TO ME\n",
    "# def compute_avg_spectrum_module(df: pd.DataFrame) -> np.ndarray:\n",
    "    \n",
    "#     avg_spec = []\n",
    "#     for i, r in df.iterrows():\n",
    "        \n",
    "#         # Convert the image to grayscale\n",
    "#         img = Image.open(r['path']).convert('L')\n",
    "        \n",
    "#         # Crop the image\n",
    "#         height, width = img.size\n",
    "#         left = (width - 224)/2\n",
    "#         top = (height - 224)/2\n",
    "#         right = (width + 224)/2\n",
    "#         bottom = (height + 224)/2\n",
    "#         img = np.array(img.crop((left, top, right, bottom)))\n",
    "\n",
    "#         # Create the median blurred image\n",
    "#         blur_img = cv2.medianBlur(img, 5)\n",
    "\n",
    "#         # High pass filtering\n",
    "#         hp_img = cv2.subtract(img, blur_img)\n",
    "        \n",
    "#         # Remove DC component\n",
    "#         #hp_img = cv2.subtract(hp_img, hp_img.mean())\n",
    "\n",
    "#         # Compute the FFT\n",
    "#         avg_spec.append(hp_img[np.newaxis, :, :])\n",
    "    \n",
    "#     # Compute the average\n",
    "#     avg_spec = np.squeeze(np.mean(avg_spec, axis=0))\n",
    "    \n",
    "#     # Compute the spectrum\n",
    "#     avg_spec = np.fft.fftshift(np.fft.fft2(avg_spec))\n",
    "    \n",
    "#     # Return the module\n",
    "#     return np.abs(avg_spec)\n",
    "\n",
    "def compute_avg_spectrum_module(df: pd.DataFrame, model: torch.nn.Module, device: str) -> np.ndarray:\n",
    "    \n",
    "    avg_spec = []\n",
    "    for i, r in df.iterrows():\n",
    "        \n",
    "        # Convert the image to grayscale\n",
    "        img = Image.open(r['path']).convert('RGB')\n",
    "        \n",
    "        # Crop the image\n",
    "        height, width = img.size\n",
    "        left = (width - 224)/2\n",
    "        top = (height - 224)/2\n",
    "        right = (width + 224)/2\n",
    "        bottom = (height + 224)/2\n",
    "        img = np.array(img.crop((left, top, right, bottom)))\n",
    "        \n",
    "        # Convert the img to grayscale\n",
    "        img = img.astype(np.float32)\n",
    "        img = (0.299 * img[:, :, 0] + 0.587 * img[:, :, 1] + 0.114 * img[:, :, 2])\n",
    "        \n",
    "        # Convert the dynamic to 0-1\n",
    "        img /= 256.0\n",
    "\n",
    "        # De-noised image\n",
    "        dn_img = model(torch.Tensor(img).unsqueeze(0).unsqueeze(0).to(device)).squeeze().cpu().numpy()\n",
    "        \n",
    "        # High-pass filtered img\n",
    "        hp_img = img-dn_img\n",
    "        \n",
    "        # Compute the FFT\n",
    "        spec = np.abs(np.fft.fftshift(np.fft.fft2(hp_img)))\n",
    "        avg_spec.append(spec[np.newaxis, :, :])\n",
    "    \n",
    "    # Compute the average\n",
    "    avg_spec = np.squeeze(np.mean(avg_spec, axis=0))\n",
    "    \n",
    "    # Return the module\n",
    "    return avg_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f7a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load the network and device\n",
    "from utils.third_party.KAIR_master.models.network_dncnn import DnCNN as net\n",
    "\n",
    "# prepare the device\n",
    "gpu = 3\n",
    "device = f'cuda:{gpu}' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load the network\n",
    "model_path = '/nas/public/exchange/JPEG-AI/code/utils/third_party/KAIR_master/model_zoo'\n",
    "nb = 20\n",
    "model_name = 'dncnn_gray_blind.pth'\n",
    "model = net(in_nc=1, out_nc=1, nc=64, nb=nb, act_mode='R')\n",
    "# model = net(in_nc=n_channels, out_nc=n_channels, nc=64, nb=nb, act_mode='BR')  # use this if BN is not merged by utils_bnorm.merge_bn(model)\n",
    "model.load_state_dict(torch.load(os.path.join(model_path, model_name)), strict=True)\n",
    "model.eval()\n",
    "for k, v in model.named_parameters():\n",
    "    v.requires_grad = False\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725707ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "fsize=20\n",
    "avg_specs = dict()\n",
    "for dataset in tqdm(all_images['dataset'].unique()):\n",
    "    # Select the dataset\n",
    "    dataset_df = all_images.loc[all_images['dataset']==dataset]\n",
    "    # Get the uncompressed sample\n",
    "    unc_df = dataset_df.loc[dataset_df['compressed']==False]\n",
    "    sample = unc_df.sample(50, random_state=42)\n",
    "    # Get the compressed samples\n",
    "    comp_df = dataset_df.loc[dataset_df['compressed']]\n",
    "    # Merge the dataframes on 'filename' and 'content'\n",
    "    sample['filename'] = sample['filename'].apply(lambda x: x.replace('jpg', 'png'))\n",
    "    comp_samples = pd.merge(comp_df, sample, left_on=['filename', 'content'], right_on=['filename', 'content'],\n",
    "                        suffixes=('', '_uncompressed'))\n",
    "    # Compute them\n",
    "    avg_specs[dataset] = dict()\n",
    "    # Compressed samples\n",
    "    for idx, t_bpp in enumerate(comp_samples['target_bpp'].unique()):\n",
    "        row_idx = (idx+1)//3\n",
    "        col_idx = idx+1 if idx+1 < 3 else idx-3+1\n",
    "        spec = compute_avg_spectrum_module(comp_samples.loc[comp_samples['target_bpp']==t_bpp], model, device)\n",
    "        avg_specs[dataset][t_bpp] = spec\n",
    "    # Uncompressed samples\n",
    "    spec = compute_avg_spectrum_module(sample, model, device)\n",
    "    avg_specs[dataset]['unc'] = spec\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ca493f",
   "metadata": {},
   "source": [
    "Plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5280c865",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fsize = 20\n",
    "for dataset, specs in avg_specs.items():\n",
    "    # Prepare the figure\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(9*3, 9*2))\n",
    "    for idx, t_bpp in enumerate(comp_samples['target_bpp'].unique()):\n",
    "        row_idx = (idx+1)//3\n",
    "        col_idx = idx+1 if idx+1 < 3 else idx-3+1\n",
    "        qmin, qmax = np.quantile(specs[t_bpp].flatten(), [0.01, 0.999])\n",
    "        axs[row_idx][col_idx].imshow(specs[t_bpp], vmax=qmax)\n",
    "        #axs[row_idx][col_idx].imshow(np.log(1+specs[t_bpp]))\n",
    "        axs[row_idx][col_idx].axis('off')\n",
    "        axs[row_idx][col_idx].set_title(f'{t_bpp} BPP avg spectrum', fontsize=fsize-5)\n",
    "    qmin, qmax = np.quantile(specs['unc'].flatten(), [0.01, 0.999])\n",
    "    axs[0][0].imshow(specs['unc'], vmax=qmax)\n",
    "    #axs[0][0].imshow(np.log(1+specs['unc']))\n",
    "    axs[0][0].axis('off')\n",
    "    axs[0][0].set_title(f'Uncompressed avg spectrum', fontsize=fsize-5)\n",
    "    fig.suptitle(f'{dataset} average spectrums', fontsize=fsize)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceb5689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
